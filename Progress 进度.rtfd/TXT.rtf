{\rtf1\ansi\ansicpg1252\cocoartf2639
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fnil\fcharset134 STSongti-SC-Regular;\f1\froman\fcharset0 TimesNewRomanPSMT;\f2\froman\fcharset0 TimesNewRomanPS-BoldMT;
\f3\froman\fcharset0 TimesNewRomanPS-ItalicMT;\f4\froman\fcharset0 TimesNewRomanPS-BoldItalicMT;}
{\colortbl;\red255\green255\blue255;\red0\green0\blue0;}
{\*\expandedcolortbl;;\cssrgb\c0\c0\c0;}
\margl1440\margr1440\vieww9980\viewh15880\viewkind1
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f0\fs24 \cf0 \'bc\'c7\'c2\'bc\'d4\'c4\'b6\'c1\'a1\'a2\'b4\'fa\'c2\'eb\'b5\'c8\'bd\'f8\'b6\'c8
\f1 \
\

\f0 \'b5\'da\'d2\'bb\'c6\'aa
\f1  A Baseline for Detecting Misclassified and Out-of-Distribution Examples in Neural Networks 
\f0 \'b4\'ed\'ce\'f3\'b7\'d6\'c0\'e0\'bb\'f9\'d7\'bc\'cf\'df
\f1 \
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\qc\partightenfactor0

\f2\b \cf0 ABSTRACT
\f1\b0 \
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
\
The 
\f2\b methodology
\f1\b0  is to use the probabilities from the SoftMax Distributions. \
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f2\b \cf0 Rationale
\f1\b0 : The correctly classified examples tend to have greater maximum probabilities than erroneously classified OOD examples. \
\

\f2\b The method
\f1\b0  is that the paper test the performance over many tasks in various fields\
\
The 
\f2\b result
\f1\b0  is a baseline to determine what is an OOD\
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\qc\partightenfactor0

\f2\b \cf0 1 INTRODUCTION\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
Gaussians Noise
\f1\b0 : named after Carl Gauss, is a term from signal processing theory denoting a kind of signal noise that has a probability density function equal to that of the normal distribution (which is also known as the Gaussian distribution). \
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\qc\partightenfactor0
\cf0 {{\NeXTGraphic Screen Shot 2022-08-29 at 6.21.32 PM.png \width4160 \height1360 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\qc\partightenfactor0
\cf0 \
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f2\b \cf0 Contribution 1: 
\f1\b0 \cf2 \expnd0\expndtw0\kerning0
This new method evaluates the 
\f2\b quality of a neural network\'92s input reconstruction
\f1\b0  to determine if an example is abnormal. \
\

\f2\b \cf0 \kerning1\expnd0\expndtw0 Contribution 2: 
\f1\b0 Another contribution of this work is the designation of 
\f2\b standard tasks and evaluation
\f1\b0  metrics for assessing the automatic detection of errors and out-of-distribution examples. \
\
In summary, while softmax classifier probabilities are not directly useful as confidence estimates, estimating model confidence is not as bleak as previously believed. This paper 
\f2\b creates a strong baseline 
\f1\b0 for detecting errors and out-of-distribution examples which we hope future research surpasses. \
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\qc\partightenfactor0

\f2\b \cf0 2 PROBLEM FORMULATION AND EVALUATION\
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 Problem 1: 
\f1\b0 The first is 
\f2\b error and success prediction
\f1\b0 . Can we predict whether a trained classifier will 
\f2\b make an error
\f1\b0  on a particular held-out test example\'92 can we predict if it will 
\f2\b correctly classify said examples
\f1\b0 ?\
\

\f2\b Problem 2: 
\f1\b0 The second is 
\f2\b in- and out-of-distribution detection
\f1\b0 . Can we predict 
\f2\b whether a test example is from a different distribution
\f1\b0  from the training data; can we predict 
\f2\b if it is from within the same
\f1\b0  
\f2\b distribution
\f1\b0 ? \
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\qc\partightenfactor0

\f2\b \cf0 \ul \ulc0 To evaluate the solution, this paper uses two evaluation metrics. \
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \ulnone Trade-off: 
\f1\b0 The score threshold depends upon the trade-off between false negatives (fn) and false positive (fp). \
\

\f2\b Model employed: 
\f1\b0 The Area Under the Receiver Operating Characteristics curve (AUROC) metric, which is a threshold-independent performance evaluation. \
\

\f2\b AUROC (Area Under the Receiver Operating Characteristics Curve): 
\f1\b0 Is a measurement of the performance of classification model. It is threshold independent. \
\

\f2\b AUPR (Area Under the Precision-Recall Curve): 
\f1\b0 Sometimes deemed more informative. It is similar idea but with two other rate which is recall and precision. \
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\qc\partightenfactor0

\f2\b \cf0 3 SOFTMAX PREDICTION PROBABILITY AS A BASELINE\
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 Maximum softmax probability 
\f1\b0 is simply the probability that the predicted class has. \
\
For example, the output of the neural networks is [10, 5, 2], corresponding to the class 0, 1, 2 accordingly. Then we use the softmax function to convert it into an array of possibility [0.99, 0.009, 0.001]. \
\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0
\cf0 {{\NeXTGraphic Pasted Graphic.jpg \width2000 \height2660 \appleattachmentpadding0 \appleembedtype0 \appleaqc
}¬}\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0
\cf0 \
\
Then we can see that the msp for the correctly classified and incorrectly classified have very 
\f2\b different msp distribution
\f1\b0 . \
\
In these two groups we also get the area under the PR and ROC curves. Basically these areas are used to determine if the model is doing great. \
\
\

\f0 \'b5\'da\'b6\'fe\'c6\'aa
\f1  Enhancing The Reliability of Out-of-distribution Image Detection in Neural Networks 
\f0 \'d4\'f6\'c7\'bf\'b4\'ed\'ce\'f3\'b7\'d6\'c0\'e0\'d4\'da\'c9\'f1\'be\'ad\'cd\'f8\'c2\'e7\'d6\'d0\'b5\'c4\'bf\'c9\'d0\'c5\'b6\'c8
\f1 \
\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\qc\partightenfactor0

\f2\b \cf0 ABSTRACT\
\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0

\f1\b0 \cf0 It 
\f2\b proposes
\f1\b0  
\f2\b ODIN
\f3\i\b0 , 
\f1\i0 which does not require any change to a pre-trained neural network. 
\f3\i \
\

\f2\i0\b ODIN is based on 
\f1\b0 the observation that using temperature scaling and adding small perturbations to the input can separate the softmax score distributions between in- and out-of-distribution images, allowing for more effective detection. \
\

\f2\b Advantage: 
\f1\b0 It is compatible with diverse network architectures and datasets. It consistently outperforms the baseline approach (from the first paper) by a large margin. \
\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\qc\partightenfactor0

\f2\b \cf0 1 INTRODUCTION\
\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0
\cf0 Based on 
\f1\b0 Hendrrycks & Gimpel\'92s work that a well-trained neural network 
\f2\b tends to assign higher softmax scores 
\f1\b0 to in-distribution examples than out-of-distribution examples. \
\

\f2\b Contribution: 
\f1\b0 We make the gap between in- and out-of-distribution examples further enlarged by 
\f2\b adding
\f1\b0  
\f2\b controlled perturbations
\f1\b0 . By three bullet points, the contributions are listed:\
\

\f2\b 1. 
\f1\b0 It proposes a simple and effective method, ODIN (
\f2\b O
\f1\b0 ut-of-
\f2\b DI
\f1\b0 stribution detector for 
\f2\b N
\f1\b0 eural networks)
\f2\b \
\
2. Tests the ODIN on state-of-art 
\f1\b0 network architectures under a diver set of in- and out-distribution dataset pairs. 
\f2\b \
\
3. Empirical analyze how 
\f1\b0 parameter settings affect the performance. \
\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\qc\partightenfactor0

\f2\b \cf0 2 PROBLEM STATEMENT
\f1\b0 \
\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0
\cf0 Assume 
\f2\b Px 
\f1\b0 and 
\f2\b Qx 
\f1\b0 denote two distinct data distributions defined on the image space 
\f2\b X
\f1\b0 . \
\

\f4\i\b f  
\f1\i0\b0 is trained on a dataset drawn from the distribution Px. \
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\qc\partightenfactor0
\cf0 \
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0

\f2\b \cf0 Temperature Scaling: 
\f1\b0 I think it is a denominator under the predicated output value from the neural network given input x. It\'92s claimed to be able to 
\f2\b distill
\f1\b0  the knowledge in neural networks and 
\f2\b calibrate 
\f1\b0 the prediction confidence in classification tasks. \
\

\f2\b Input Preprocessing: 
\f1\b0 In addition to temperature scaling, this paper preprocess the input by adding small perturbations: \
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\qc\partightenfactor0
\cf0 [SUPER COMPLEX EQUATION]\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0
\cf0 The rationale behind the preprocessing is based on Goodfellow\'92s work, where small perturbations are added to decrease the softmax score for the true label and force the neural network to make a wrong prediction. \
\
However, in this paper, they aim to increase the softmax score of any given input, without the need for a class label at all. As a result, 
\f2\b the perturbation can have stronger effect on the in-distribution images than that on out-of-distribution images. 
\f1\b0 (The perturbations can be easily computed by back-propagating the gradient of the cross-entropy loss w.r.t the input).\
\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\qc\partightenfactor0

\f2\b \cf0 3 ODIN:OUT-OF-DISTRIBUTION DETECTOR\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0
\cf0 \
Out-of-distribution Detector: 
\f1\b0 For each image 
\f2\b x
\f1\b0 , the research first calculate the preprocessed image 
\f2\b x ~. 
\f1\b0 Next, the researcher feed the preprocessed image into the neural network, calculate its calibrated softmax score and compare the score to the threshold. If it\'92s larger than the threshold, it\'92s classified as in-distribution. \
\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\qc\partightenfactor0
\cf0 4 EXPERIMENTS\
\
[TOO COMPLEX]\
\
5 DISCUSSIONS\
\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\qc\partightenfactor0
\cf0 [TOO COMPLEX]\
\
6 RELATED WORKS AND FUTURE DIRECTIONS\
\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0

\f2\b \cf0 Other OOD Detection Methods: \
\
Density estimation: 
\f1\b0 It uses probabilistic models to estimate the in-distribution density and declares a test example to be out-of-distribution if it locates in the low-density areas. \
\

\f2\b Clustering method: 
\f1\b0 It is based on the statistical distance, and declares an example to be out-of-distribution if it locates far from its neighborhood. \
\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\qc\partightenfactor0
\cf0 (Known to be unreliable in high-dimensional space such as image space)\
\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0

\f2\b \cf0 - Generative adversarial networks\
\
- Convolutional network\
\
- Transfer representation-learning \
\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\qc\partightenfactor0

\f1\b0 \cf0 All these works require enlarging or modifying ht eneural networks. \
\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0
\cf0 In the future, this paper suggests: \
\
1. ON in-distribution images, modern neural networks tend to produce outputs with larger variance across class labels, and \
\
2. Neural networks have larger norm of gradient of log-softmax scores when applied on in-distribution images. \
\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\qc\partightenfactor0

\f2\b \cf0 7 CONCLUSIONS\
\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0
\cf0 \
}